<!DOCTYPE HTML>
<!--
	Alpha by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Researches</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<header id="header">
					<!--h1><a href="index.html">Alpha</a> by HTML5 UP</h1-->
					<nav id="nav">
						<ul>
							<li><a href="index.html">Home</a></li>
							<li>
								<a href="#" class="icon solid fa-angle-down">Menu</a>
								<ul>
									<li><a href="performance.html">Performances</a></li>
									<li><a href="research.html">Researches</a></li>
									<li><a href="teaching.html">Teachings</a></li>
									<li><a href="idea.html">Ideas</a></li>
								</ul>
							</li>
						</ul>
					</nav>
				</header>

			<!-- Main -->
				<section id="main" class="container">
					<header>
						<h2>Researches</h2>
						<p>I elaborate the works I have done. 
							<br>
							The description of projects are in plain words.
							<br>
							I hope it will be pleasant to read about my journey.
							</p>
					</header>
					<div class="box">
						<span class="image featured"><img src="images/pic01.jpg" height = 50 /></span>
						<!--div style="font-size: 20px"-->

						<h3><b>Menu</b></h3>
					
						<br>
						
						<a href="#r1">R1--Speech enhancement based on deep learning and beamforming</a>
						<br>
						<a href="#r2">R2--Music beat extraction for symbolic music based on language model</a>
						<br>
						<a href="#r3">R3--Speech enhancement internship based on microphone design and transfer learning</a>
						<br>
						<a href="#r4">R4--Music video understanding based on language model</a>
						<br>
						<a href="#r5">R5--Speech synthsize based on tone extraction</a>
						<br>
						<a href="#r6">R6--Obstructive Sleep Apnea diagnosis based on 3D rendering</a>
						
					</div>
					<div class="box" id="r1">
						<h3><b>R1--Speech enhancement based on deep learning and beamforming</b></h3>
						<p> 
						My interest in Digital Signal Processing started when I was an undergraduate student. While studying in Shanghai Jiao
						Tong University, I registered for a handful of courses concentrating on DSP techniques. These courses not only helped me
						understand the basics of DSP algorithms, but also provided me a chance to see a lot of exciting and edge-cutting DSP
						theories and applications. Realizing that DSP techniques are important in both the industry and our daily life, I
						selected my capstone project focusing on speech enhancement for multiple channel inputs. In this project, I applied both
						statistical machine learning methods (Expectation Maximization) and deep learning methods (LSTM) for audio signal mask
						estimation. To better adapt real-world application scenarios, I also implemented the source separation algorithm for
						multi-channel microphone arrays. Eventually, I deployed my algorithms on a Samsung mobile phone with 3 microphones, and
						I was rejoiced when I saw my codes worked well during the tests. 
						
						

						<br>
						</p>
					</div>

					<div class="box" id="r2">
						<h3><b>R2--Music beat extraction for symbolic music based on language model </b></h3>
						<p> 
						Aiming at developing better DSP algorithms and creating meaningful DSP applications, I joined Music X Lab as a research
						assistant after obtaining my Bachelor’s degree. In the Music X Lab, I realized that it is both meaningful and
						challenging to have machines understand music recordings. Consequently, I selected music beat analysis as my research
						topic. To complete this project, I familiarized myself with symbolic music and language models. Also, I completed beat
						tracking tasks and modified MuseBERT for music beat estimation. These proposed methods allowed computers to
						intelligently understand music recordings based on music beats. 
						
						
						<br>
						<br>
						</p>
					</div>

					<div class="box" id="r3">
						<h3><b>R3--Speech enhancement internship based on microphone design and transfer learning</b></h3>
						<p> 
						Although the music processing project turned out to be a success, the more I worked, the more I found myself lacking
						theoretical and practical knowledge, especially given the fact that DSP techniques are revolutionized everyday with
						modern deep learning methods. Therefore, I decided to join Georgia Tech and study towards a master’s degree. To explore
						more about DSP and music analysis, I joined the Music Informatics Lab and continued my research in relevant fields.
						During my first year of my master’s study, I developed a deep learning model that could automatically rank music
						performances and give performance scores. This project again made an important contribution to music understanding with
						computers. To catch up with the up-to-date industrial developments, I completed two internships in the area of signal
						processing and music processing. In 2023, I joined shure.inc, where I applied speech enhancement (beam-forming and deep
						learning) in microphone systems for professional recording facilities. Plus, I also learned about sound simulation and
						microphone design. 
						
						
						<br>
						</p>
					</div>

					<div class="box" id="r4">
						<h3><b>R4--Music video understanding based on language model</b></h3>
						<p>
						In 2024, I completed another internship at clearmotion.inc, where I created an in-vehicle
						infotainment system that can automatically perform vehicle chassis movements according to videos played on the phone.
						This is the most meaningful research experience I have ever partipated in.
						During this internship, I designed and implemented a novel in-vehicle infotainment system that could automatically
						generate 4-D movies using motion-enabled vehicle seats. This system functions as follows: To begin with, it analyzes the
						movies played on the user’s phone or on the dashboard display. Then, critical moments in the movies are detected,
						including car crashes, explosions and so on. For each critical scene, the infotainment system assigns a set of seat
						movements (e.g., shaking or sliding). Finally, control commands are sent to the hardware and the seat completes those
						movements in real time.
						
						The most challenging part in this project is to automatically detect if and when there is a critical scene. Although
						there are many 4D movies in the theater, most of their physical effects have been designed manually. Also, the datasets
						for movies are insufficient in labels and descriptions, despite the abundant raw data from different video-sharing
						platforms. Consequently, I have to develop my own algorithms and gather my own datasets for my specific tasks.
						Fortunately, current Deep Learning approaches including video understanding, video captioning and video teaser
						generation are related to my tasks and can be adopted to my specific applications. Since the background music and the
						video scenery are highly corelated, I decided to first analyze the music and the video separately and then combine the
						results together, such that I could improve the accuracy in the detection of critical movie segments. To analyze the
						videos, I broke down the tasks into keyframe extraction, teaser narration generation, keyword clustering and sound event
						detection. For every second of video segments, 2-4 keyframes were extracted. Then, the objects in the video frames were
						identified and the main ideas of the video segments were summarized. By applying a clustering algorithm, the
						dependencies between these segments could be revealed. Finally, a sound event detection model is called to rule out
						false positive results. For audio signal analysis, I applied a separation model to remove the background music as well
						as extract sound effects and dialogues. By applying a beat extraction model, both the regular and irregular rhythm
						patterns of the audio were obtained, providing a rough segmentation of the audios. Lastly, beat extraction algorithm was
						called again to refine the results for critical events. For each task, one or more deep neural networks have been
						trained, and I also developed rule-based algorithms to analyze movie transitions and integrate audio and video analysis.
						With these analysis results, seat motions were assigned and as well as executed, turning a traditional movie into a 4-D
						movie on the vehicle seat. I further designed a configuration software which allowed the users to define their preferred
						seat motions. Field tests showed that my system worked as expected, and the company is currently preparing my results
						for a patent as well as a commercial product.
						
						
						This internship experience is meaningful and impressive for three reasons. First, during the internship, I proposed and
						implemented novel movie analysis algorithms, which greatly improved my skills and understandings on Deep Learning and
						related fields. I read countless research papers and support documents during this project, and I could eventually
						develop my own Deep Learning models and software tools. Second, this is the first time that I complete an industry-level
						product independently, and I gathered invaluable experiences in analyzing and solving problems. My manager is not an
						expert in Deep Learning and Multimedia Processing, and he could not provide a detailed road path for this project.
						However, by dividing this abstract and high-level idea into concrete and low-level subtasks, I managed to conquer them
						separately and integrate them into a whole system. Third, the product itself is creative as well as useful, convincing
						me that modern Deep Learning techniques could really benefit our daily life.
						
						
						<br>
						</p>
					</div>

					<div class="box" id="r5">
						<h3><b>R5--Speech synthsize based on tone extraction</b></h3>
						<p>
						Most recently, I got a chance to connect with Georgia Tech professors working on healthcares using advanced signal
						processing and machine learning techniques. Inspired by them, I started to understand how DSP methods could contribute
						to human health, which is of great significance for the quality of life and social welfare. Last semester, I worked on
						voice conversion for the patients with vocal cord diseases during the therapy stage. I extracted the short syllables
						which are good in quality and extracted the tone of those moments to synthesize a better speech for the patient. 
						<br>
						</p>
					</div>
					
					<div class="box" id="r6">
						<h3><b>R6--Obstructive Sleep Apnea diagnosis based on 3D rendering</b></h3>

						<p> 
							
						This semester, I am working on a 3D reconstruction for the human airway before surgery based on CT scans and 2D photos. I
						think only the combination of the embeddings for reconstruction and metadata might be of help in diagnosis, which is
						verified during the experiments. While doing these two projects, I realized how modern technologies could help people
						and lead us to a better life. I believe this is the ultimate goal of technology development, and I wish I could continue
						doing this in the future. 
							
							
						</p>

					
						
	
					<span class="image featured"><img src="images/pic01.jpg" height = 50 /></span>





					</div>
				</section>

			<!-- Footer -->
				<footer id="footer">
					<ul class="copyright">
						<li>&copy; Jingyan(Joy) Xu. All rights reserved.</li>
					</ul>
				</footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>